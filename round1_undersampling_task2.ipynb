{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "7c27ecd8-cadf-45e6-9a0a-3543fb627261",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "round1_undersampling_task2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mantasbandonis/CHEERS-challenge/blob/main/round1_undersampling_task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LmqnG60IUDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add9c738-7234-4dd2-ff99-1c55c926b58d"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install imbalanced-learn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 22.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 34.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaJC7U9mA6LI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acef3c35-b111-4d4a-ccd1-c259e5e90298"
      },
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_curve, auc\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9mXKBTAJ8mp",
        "outputId": "60767c63-86b5-4d75-959d-d379f6cb1b2a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00001-78bff357-c978-48c5-89d8-153406a65001",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1710,
        "execution_start": 1619186697861,
        "source_hash": "8935307",
        "id": "aCPyaFWdH4lg"
      },
      "source": [
        "documents_train = pd.read_csv(\"/content/drive/MyDrive/cheers_challenge/round1/data_round_1/documents_en_train.csv\")\n",
        "sentences_train = pd.read_csv(\"/content/drive/MyDrive/cheers_challenge/round1/data_round_1/sentences_en_train.csv\")\n",
        "\n",
        "documents_val = pd.read_csv(\"/content/drive/MyDrive/cheers_challenge/round1/data_round_1/documents_en_val.csv\")\n",
        "sentences_val = pd.read_csv(\"/content/drive/MyDrive/cheers_challenge/round1/data_round_1/sentences_en_val.csv\")\n",
        "\n",
        "documents_test = pd.read_csv(\"/content/drive/MyDrive/cheers_challenge/round1/data_round_1/documents_en_test.csv\")\n",
        "sentences_test = pd.read_csv(\"/content/drive/MyDrive/cheers_challenge/round1/data_round_1/sentences_en_test.csv\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cheers_challenge/round1/data_round_1/word_embedding.txt\", \"rb\") as fp: \n",
        "    embedding_distilbert = pickle.load(fp)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cheers_challenge/round1/data_round_1/attention_masks.txt\", \"rb\") as fp: \n",
        "    attention_masks = pickle.load(fp)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cheers_challenge/round1/data_round_1/word_embedding_val.txt\", \"rb\") as fp: \n",
        "    embedding_distilbert_val = pickle.load(fp)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cheers_challenge/round1/data_round_1/attention_masks_val.txt\", \"rb\") as fp: \n",
        "    attention_masks_val = pickle.load(fp)\n",
        "\n",
        "#immap_sector_name_to_id.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VtA2ygc6nXs"
      },
      "source": [
        "def process_sector_ids(sentences):\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    sectors = pd.DataFrame(mlb.fit_transform(sentences[\"sector_ids\"]),columns=mlb.classes_)\n",
        "    sectors = sectors.drop([\",\", \"[\", \"]\", \" \"], axis = 1)\n",
        "    sectors[\"-1\"] = 0\n",
        "    sectors[\"-1\"][sectors.sum(axis=1) == 0] = 1\n",
        "\n",
        "    return sectors.values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1eLuGbt9OWf"
      },
      "source": [
        "sectors_train = process_sector_ids(sentences_train)\n",
        "sectors_val = process_sector_ids(sentences_val)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfBxI8Cl0l48"
      },
      "source": [
        "y = sectors_train"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFgGNSGB8YEA"
      },
      "source": [
        "def undersampler(embedding, attention_mask, y):\n",
        "    '''\n",
        "    Given the embedding, attention masks and y returns undersampled versions \n",
        "    of the input variables\n",
        "    '''\n",
        "\n",
        "    nr_y1 = len(pd.DataFrame(y)[y==1])\n",
        "    y_0_idx = np.asarray(pd.DataFrame(y)[y==0].index).flatten()\n",
        "\n",
        "    print(\"Nr samples class 0:\", len(y_0_idx))\n",
        "    print(\"Nr samples class 1:\", nr_y1)\n",
        "\n",
        "    idx_list = np.random.choice(y_0_idx, size = nr_y1)\n",
        "\n",
        "    embedding_0 = embedding_distilbert[idx_list]\n",
        "    masks_0 = np.array(attention_masks)[idx_list]\n",
        "    y_0 = y[idx_list].values\n",
        "\n",
        "    if (len(embedding_0) == len(masks_0) & len(embedding_0) == len(y_0)):\n",
        "        print(\"Nr undersampled samples class 0:\", len(embedding_0))\n",
        "\n",
        "    y_1_idx = np.asarray(pd.DataFrame(y)[y==1].index).flatten()\n",
        "    \n",
        "    embedding_1 = embedding_distilbert[y_1_idx]\n",
        "    masks_1 = np.array(attention_masks)[y_1_idx]\n",
        "    y_1 = y[y_1_idx].values\n",
        "\n",
        "\n",
        "    embedding_undersampled = np.concatenate((embedding_0, embedding_1))\n",
        "    masks_undersampled = np.concatenate((masks_0, masks_1))\n",
        "    y_undersampled = np.concatenate((y_0, y_1))\n",
        "\n",
        "    if (len(embedding_undersampled) == len(masks_undersampled) & len(embedding_undersampled) == len(y_undersampled)):\n",
        "        print(\"\\nSize undersampled dataset:\", len(embedding_undersampled))\n",
        "\n",
        "    print(\"Done!\")\n",
        "    return embedding_undersampled, masks_undersampled, y_undersampled"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32V35__5UaAa",
        "outputId": "015d27a7-1b42-4f53-8b62-f951aa4d02a2"
      },
      "source": [
        "y[:30]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oWpukyad27g",
        "outputId": "b5617f5f-4e66-4df9-9f35-7a7b441594d3"
      },
      "source": [
        "print(sectors_train.shape)\n",
        "print(sectors_val.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(261981, 11)\n",
            "(37109, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "9S8EsCc3E1by",
        "outputId": "72c9be7e-75f3-48c4-ae48-0cff6f06a9f6"
      },
      "source": [
        "embedding_distilbert, attention_masks, y = undersampler(embedding_distilbert, attention_masks, y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nr samples class 0: 2581283\n",
            "Nr samples class 1: 300508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a63323cdbf71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_distilbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mundersampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_distilbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-f10a835d85e0>\u001b[0m in \u001b[0;36mundersampler\u001b[0;34m(embedding, attention_mask, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0membedding_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_distilbert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmasks_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0my_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MGnryLc_79M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf91091-af4d-485a-a84a-7faff5e929cd"
      },
      "source": [
        "nr_classes = y.shape[1]\n",
        "print(\"nr_classes:\", nr_classes)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nr_classes: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IROh24iL0IGn"
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased')\n",
        "input_ids = torch.tensor(embedding_distilbert)\n",
        "attention_mask = torch.tensor(attention_masks)\n",
        "labels = torch.tensor(y)\n",
        "data_size = len(labels)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47n-dkXxfXGD",
        "outputId": "088e5cf6-2761-4405-e2c8-e3394f6f0a22"
      },
      "source": [
        "print(labels.shape)\n",
        "print(input_ids.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([261981, 11])\n",
            "torch.Size([261981, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXmrCjXZB5Mq"
      },
      "source": [
        "batchsize = 8\n",
        "def get_batches(input_ids, attention_mask, labels, batch_size=batchsize):\n",
        "\n",
        "    tensor_dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, labels)    \n",
        "    tensor_randomsampler = torch.utils.data.RandomSampler(tensor_dataset)    \n",
        "    tensor_dataloader = torch.utils.data.DataLoader(tensor_dataset, sampler=tensor_randomsampler, batch_size=batch_size)    \n",
        "    return tensor_dataloader"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVQnsSzhCxj4"
      },
      "source": [
        "batch_train = get_batches(input_ids, attention_mask, labels)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtIDIrgLG9UP"
      },
      "source": [
        "#validation dataloader\n",
        "y_val = sectors_val\n",
        "y_val = torch.tensor(y_val)#.to(device)\n",
        "input_ids_val = torch.tensor(embedding_distilbert_val)#.to(device)\n",
        "attention_mask_val = torch.tensor(attention_masks_val)#.to(device)\n",
        "\n",
        "batch_size = 8\n",
        "#test_dataset = TensorDataset(input_ids_val, attention_mask_val)\n",
        "#test_sampler = SequentialSampler(test_dataset)\n",
        "#test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=8)\n",
        "\n",
        "val_data = TensorDataset(input_ids_val, attention_mask_val, y_val)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, \n",
        "                            sampler=val_sampler, \n",
        "                            batch_size=batch_size,\n",
        "                            num_workers= 0)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egoSJWArGcKK"
      },
      "source": [
        "def eval_bert(model, validation_dataloader):\n",
        "    preds = torch.tensor([]).to(\"cpu\")\n",
        "    with torch.no_grad():\n",
        "        i = 0\n",
        "        for b_input_ids, b_input_mask, b_labels in validation_dataloader:\n",
        "            if i % 1000 == 0:\n",
        "                print(\"Batches validated:\", i)\n",
        "            i+=1\n",
        "            b_input_ids = b_input_ids.to(device)\n",
        "            b_input_mask = b_input_mask.to(device)\n",
        "            b_labels = b_labels.to(device)\n",
        "            model.eval()\n",
        "\n",
        "            output = model(b_input_ids, b_input_mask)\n",
        "            y_hat = output.logits.to(\"cpu\")\n",
        "            del b_input_ids, b_input_mask, b_labels, output\n",
        "\n",
        "            preds = torch.cat((preds, y_hat))\n",
        "            del y_hat\n",
        "            \n",
        "        #probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "    print(\"Validation done!\")\n",
        "    return preds"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEYR7EFsHqLv"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    print('F1 score:', f1)\n",
        "\n",
        "    cm=confusion_matrix(y_true,y_pred)\n",
        "    print(cm)\n",
        "    cm = pd.DataFrame(cm, index = [0,1], columns = [0,1])\n",
        "    sns.heatmap(cm, annot=True, fmt = '.0f')\n",
        "    plt.show()\n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc0qbCOpDYMH"
      },
      "source": [
        "def train_model(batch, model, optimizer, scheduler, epochs, device, batchsize):    \n",
        "    model.train()  # Set the mode to training    \n",
        "    for e in range(epochs):\n",
        "        epochs_loss = []        \n",
        "        for i, batch_tuple in enumerate(batch):            \n",
        "            batch_tuple = (t.to(device) for t in batch_tuple)            \n",
        "            input_ids, attention_mask, labels = batch_tuple            \n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)            \n",
        "            loss = outputs[0]\n",
        "            epochs_loss.append(loss.item())\n",
        "            logits = outputs[1]\n",
        "            hidden_states_output = outputs[2]\n",
        "            attention_mask_output = outputs[3]\n",
        "            steps = 1000\n",
        "            if i % steps == 0:\n",
        "                if i == 0:\n",
        "                    avg_loss = loss.item()\n",
        "                else:\n",
        "                    avg_loss = np.mean(epochs_loss[-steps:])\n",
        "                \n",
        "                depleted = round(100*i*batchsize/data_size, 2)\n",
        "                print(\"Avg. Train Loss: {0}, Step: {1}, depleted: {2}%\".format(round(avg_loss,3), i, depleted))            \n",
        "            \n",
        "            model.zero_grad()            \n",
        "            optimizer.zero_grad()         \n",
        "            loss.backward()            \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), parameters['max_grad_norm'])            \n",
        "            optimizer.step()            \n",
        "            scheduler.step()\n",
        "        \n",
        "        model.save_pretrained('/content/drive/MyDrive/cheers_challenge/round1/models/distilbert_task2_epoch_'+str(e))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P5ezlsAGoCX"
      },
      "source": [
        "def train_eval(batch, model, optimizer, scheduler, epochs, device, batchsize, validation_dataloader):    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        epochs_loss = []\n",
        "        print(\"Training: epoch\", e+1)        \n",
        "        for i, batch_tuple in enumerate(batch):          \n",
        "            batch_tuple = (t.to(device) for t in batch_tuple)    \n",
        "            \n",
        "            input_ids, attention_mask, labels = batch_tuple            \n",
        "            print(\"input_ids:\", input_ids.shape)\n",
        "            print(\"attention_mask:\", attention_mask.shape)\n",
        "            print(\"labels:\", labels.shape)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                        \n",
        "            loss = outputs[0]\n",
        "            epochs_loss.append(loss.item())\n",
        "            logits = outputs[1]\n",
        "            hidden_states_output = outputs[2]\n",
        "            attention_mask_output = outputs[3]\n",
        "            steps = 1000\n",
        "            if i % steps == 0:\n",
        "                if i == 0:\n",
        "                    avg_loss = loss.item()\n",
        "                else:\n",
        "                    avg_loss = np.mean(epochs_loss[-steps:])\n",
        "                \n",
        "                depleted = round(100*i*batchsize/data_size, 2)\n",
        "                print(\"Avg. Train Loss: {0}, Step: {1}, depleted: {2}%\".format(round(avg_loss,3), i, depleted))            \n",
        "            \n",
        "            model.zero_grad()            \n",
        "            optimizer.zero_grad()         \n",
        "            loss.backward()            \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), parameters['max_grad_norm'])            \n",
        "            optimizer.step()            \n",
        "            scheduler.step()\n",
        "        \n",
        "        #save model after epoch\n",
        "        model.save_pretrained('/content/drive/MyDrive/cheers_challenge/round1/models/distilbert_task2_epoch_'+str(e))\n",
        "        print(\"Model distilbert_under_1_epoch_\"+str(e)+ \" saved!\")\n",
        "        print(\"\\n***************************************************\\n\")\n",
        "        print(\"Evaluation\")\n",
        "        #evaluate after epoch\n",
        "        probs = eval_bert(model, validation_dataloader)\n",
        "        print(\"\\n\")\n",
        "        softm_probs = F.softmax(probs).cpu().numpy()\n",
        "        evaluate_roc(softm_probs, y_val)\n",
        "        print(\"\\n---------------------------------------------------\\n\")"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEbzMI3ekHXu"
      },
      "source": [
        "self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', \n",
        "                                                      num_labels=nr_classes, \n",
        "                                                      output_hidden_states=True, \n",
        "                                                      output_attentions=True)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K4BPD_tkMwY"
      },
      "source": [
        "class BERT_MODEL(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      \n",
        "      super(BERT_MODEL, self).__init__()\n",
        "\n",
        "      self.bert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased') \n",
        "      \n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,11)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.sigmoid = torch.sigmoid\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(input_ids, attention_mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.sigmoid(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nObR2EaBEHRl"
      },
      "source": [
        "epochs=3\n",
        "parameters = {\n",
        "    'learning_rate': 1e-5,\n",
        "    'num_warmup_steps': 1500,\n",
        "    'num_training_steps': len(batch_train) * epochs,\n",
        "    'max_grad_norm': 1\n",
        "    }\n",
        "\n",
        "optimizer = transformers.AdamW(model.parameters(), lr=parameters['learning_rate'], correct_bias=False)\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer,\n",
        "                                                         num_warmup_steps=parameters['num_warmup_steps'],\n",
        "                                                         num_training_steps=parameters['num_training_steps'])"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5exwY5ukeq8p",
        "outputId": "132bfd8e-8094-478c-8297-1442989c14f1"
      },
      "source": [
        "print(\"batch_train:\", batch_train)\n",
        "print(\"batchsize:\", batchsize)\n",
        "print(\"val_dataloader:\", val_dataloader)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_train: <torch.utils.data.dataloader.DataLoader object at 0x7f785e44ff10>\n",
            "batchsize: 8\n",
            "val_dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f785e3d3450>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkYBxpPcEY_G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "bb22ba99-35e2-4807-8ad7-6c1a4112df09"
      },
      "source": [
        "model = BERT_MODEL\n",
        "model = model.to(device)\n",
        "train_eval(batch_train, model(), optimizer, scheduler, epochs, device, batchsize, val_dataloader)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-186e45cbafe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT_MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBERT_MODEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch.device' object has no attribute '_apply'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByluEtz8vofv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac636327-7ebc-4753-9a84-0476fd7c3075"
      },
      "source": [
        "attention_mask#In case you want to load the own trained BERT on the dataset:\n",
        "\n",
        "#model = DistilBertForSequenceClassification.from_pretrained('/content/drive/MyDrive/cheers_challenge/round1/models/distilbert_1')\n",
        "#model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrboLYBbipxA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}